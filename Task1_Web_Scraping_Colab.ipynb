{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41029860",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§¹ Task 1: Data Collection & Web Scraping (Google Colab)\n",
    "\n",
    "This notebook shows you how to collect data from the web using **requests**, **BeautifulSoup**, and **pandas**, handle **pagination**, and save results to **CSV/JSON**.  \n",
    "It also includes an **optional Selenium** section for sites that use dynamic (JavaScript-rendered) content.\n",
    "\n",
    "> Demo site: **Books to Scrape** (static) â€” perfect for practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316044f",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Setup (Install & Imports)\n",
    "\n",
    "Run the cell below to make sure required libraries are installed in Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567bf808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you're on Google Colab, most of these are already available.\n",
    "# This ensures everything is present & up-to-date.\n",
    "!pip -q install beautifulsoup4 lxml pandas requests --upgrade\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "print(\"Libraries ready âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f003ab2",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Helper Utilities (Polite Scraping)\n",
    "\n",
    "- Use a **User-Agent** header\n",
    "- Add small delays if needed\n",
    "- Always check the website's **robots.txt** and **Terms of Service** before scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_soup(url, wait=0.0):\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    if wait:\n",
    "        time.sleep(wait)\n",
    "    return BeautifulSoup(resp.text, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1fc752",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Static Site Demo: Books to Scrape\n",
    "\n",
    "We'll collect **book title, price, availability, product URL** across all pages.\n",
    "\n",
    "**Selectors (from Inspect Element):**\n",
    "- Each book: `article.product_pod`\n",
    "- Title: `h3 > a['title']`\n",
    "- Price: `p.price_color`\n",
    "- Availability: `p.instock.availability`\n",
    "- Detail link: `h3 > a['href']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE = \"https://books.toscrape.com/\"\n",
    "START = urljoin(BASE, \"catalogue/page-1.html\")\n",
    "\n",
    "def parse_page(soup):\n",
    "    rows = []\n",
    "    for card in soup.select(\"article.product_pod\"):\n",
    "        title = card.h3.a.get(\"title\", \"\").strip()\n",
    "        price = card.select_one(\"p.price_color\").get_text(strip=True)\n",
    "        availability = card.select_one(\"p.instock.availability\").get_text(strip=True)\n",
    "        rel_link = card.h3.a.get(\"href\", \"\")\n",
    "        product_url = urljoin(BASE + \"catalogue/\", rel_link)\n",
    "        rows.append({\n",
    "            \"title\": title,\n",
    "            \"price\": price,\n",
    "            \"availability\": availability,\n",
    "            \"product_url\": product_url\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def find_next_page(soup, current_url):\n",
    "    next_li = soup.select_one(\"li.next > a\")\n",
    "    if not next_li:\n",
    "        return None\n",
    "    href = next_li.get(\"href\")\n",
    "    # next links are relative to current page\n",
    "    return urljoin(current_url, href)\n",
    "\n",
    "all_rows = []\n",
    "page_url = START\n",
    "page_num = 1\n",
    "\n",
    "while page_url:\n",
    "    print(f\"Scraping page {page_num}: {page_url}\")\n",
    "    soup = get_soup(page_url, wait=0.3)\n",
    "    all_rows.extend(parse_page(soup))\n",
    "    page_url = find_next_page(soup, page_url)\n",
    "    page_num += 1\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "print(f\"Collected {len(df)} rows\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36145be0",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Save Data (CSV & JSON)\n",
    "\n",
    "Run to create files you can download.  \n",
    "**Tip:** If you want to save directly to Google Drive, run the next section to mount Drive first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_path = \"books_data.csv\"\n",
    "json_path = \"books_data.json\"\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
    "print(\"Saved files:\")\n",
    "print(\" -\", csv_path)\n",
    "print(\" -\", json_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c09b9e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) (Optional) Save to Google Drive\n",
    "\n",
    "Run this cell to mount your Drive and copy the output files there.\n",
    "You'll be asked to authorize the connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "target_folder = \"/content/drive/MyDrive/web_scraping_outputs\"\n",
    "import os, shutil\n",
    "\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "for p in [csv_path, json_path]:\n",
    "    shutil.copy(p, os.path.join(target_folder, os.path.basename(p)))\n",
    "\n",
    "print(\"Copied to:\", target_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ef73e",
   "metadata": {},
   "source": [
    "\n",
    "## 6) (Optional) Dynamic Content with Selenium\n",
    "\n",
    "If a site loads data with JavaScript, `requests` won't see it. Use **Selenium** to automate a headless browser and grab the rendered HTML.\n",
    "\n",
    "Below is a minimal setup that works in Colab.  \n",
    "**Demo target:** `https://quotes.toscrape.com/scroll` (AJAX infinite scroll).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install Selenium & driver manager\n",
    "!pip -q install selenium webdriver-manager --upgrade\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Configure headless Chrome (works in Colab)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "try:\n",
    "    driver.get(\"https://quotes.toscrape.com/scroll\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Scroll a few times to load more content\n",
    "    for _ in range(5):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    # Parse the rendered DOM\n",
    "    # (You can also pass driver.page_source into BeautifulSoup if you prefer)\n",
    "    quotes = driver.find_elements(By.CSS_SELECTOR, \".quote\")\n",
    "    scraped = []\n",
    "    for q in quotes:\n",
    "        text = q.find_element(By.CSS_SELECTOR, \".text\").text\n",
    "        author = q.find_element(By.CSS_SELECTOR, \".author\").text\n",
    "        scraped.append({\"text\": text, \"author\": author})\n",
    "\n",
    "    df_dyn = pd.DataFrame(scraped)\n",
    "    print(\"Dynamic rows:\", len(df_dyn))\n",
    "    df_dyn.head(10)\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce40e1e",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (Optional) Save Dynamic Results\n",
    "\n",
    "If you ran the Selenium section, run this to save the results as files too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'df_dyn' in globals():\n",
    "    df_dyn.to_csv(\"quotes_dynamic.csv\", index=False)\n",
    "    df_dyn.to_json(\"quotes_dynamic.json\", orient=\"records\", indent=2, force_ascii=False)\n",
    "    print(\"Saved quotes_dynamic.csv & quotes_dynamic.json\")\n",
    "else:\n",
    "    print(\"Skip: dynamic section wasn't run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369043d",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Tips & Troubleshooting\n",
    "\n",
    "- **Robots.txt & ToS:** Always confirm you have permission to scrape.\n",
    "- **Selectors break?** Right-click â†’ *Inspect* to re-check classes/structure.\n",
    "- **Pagination strategies:** \n",
    "  - Look for a *Next* button and follow its link (used above).\n",
    "  - Increment a `page=` query parameter until no results return.\n",
    "- **Rate limiting:** Add `time.sleep()` between requests and set headers.\n",
    "- **Errors:** Wrap requests in `try/except`, and use `response.raise_for_status()`.\n",
    "- **Prefer APIs:** If the site has a public API endpoint, use it instead of scraping.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
